---
title: "On Sparsity in Overparametrised Shallow ReLU Networks"
date: 2020-06-01
type: "preprint"
authors: "Joan Bruna, Jaume de Dios"
year: "2020"
arxiv: "2006.10225"
description: "The analysis of neural network training beyond their linearization regime remains an outstanding open question, even in the simplest setup of a single hidden-layer. The limit of infinitely wide networks provides an appealing route forward through the mean-field perspective, but a key challenge is..."
paper_url: "https://arxiv.org/abs/2006.10225"
featured_image: "/images/papers/bruna-nn.png"

---

![Featured Image](/images/papers/bruna-nn.png)

**Authors:** Joan Bruna, Jaume de Dios

**Type:** Preprint (2020)

[arXiv:2006.10225](https://arxiv.org/abs/2006.10225)

## Abstract

The analysis of neural network training beyond their linearization regime remains an outstanding open question, even in the simplest setup of a single hidden-layer.
