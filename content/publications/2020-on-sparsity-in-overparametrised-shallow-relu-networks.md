---
title: "On Sparsity in Overparametrised Shallow ReLU Networks"
date: 2020-06-01
type: "preprint"
authors: "Joan Bruna, Jaume de Dios"
year: "2020"
arxiv: "2006.10225"
paper_url: "https://arxiv.org/abs/2006.10225"
featured_image: "/images/papers/bruna-nn.png"
---

![Featured Image](/images/papers/bruna-nn.png)

**Authors:** Joan Bruna, Jaume de Dios

**Type:** Preprint (2020)

[arXiv:2006.10225](https://arxiv.org/abs/2006.10225)

## Abstract

The analysis of neural network training beyond their linearization regime remains an outstanding open question, even in the simplest setup of a single hidden-layer.
