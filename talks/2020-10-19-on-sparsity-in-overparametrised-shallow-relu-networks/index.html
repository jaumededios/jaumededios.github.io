<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>On Sparsity in Overparametrised Shallow ReLU Networks ‚Äî Jaume de Dios Pont</title>
<meta name=description content="CDS Faculty Fellow ‚Äî NYU Center for Data Science"><meta name=author content="Jaume de Dios Pont"><meta property="og:title" content="On Sparsity in Overparametrised Shallow ReLU Networks"><meta property="og:description" content="CDS Faculty Fellow ‚Äî NYU Center for Data Science"><meta property="og:type" content="website"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&family=Source+Serif+4:ital,wght@0,400;0,600;1,400&display=swap" rel=stylesheet><link rel=stylesheet href="/css/style.css?v=1771349999"><script>(function(){var e=localStorage.getItem("theme");(e==="dark"||!e&&window.matchMedia("(prefers-color-scheme: dark)").matches)&&document.documentElement.setAttribute("data-theme","dark")})()</script></head><body><nav class=nav><div class=nav-inner><a class=nav-title href=https://jaume.dedios.cat/>Jaume de Dios Pont</a><div class=nav-right><div class=nav-links id=navLinks><a href=/>About</a>
<a href=/publications/>Publications</a>
<a href=/cv.pdf target=_blank>CV</a></div><button class=theme-toggle id=themeToggle aria-label="Toggle dark mode" title="Toggle dark mode">
<span class=theme-icon>üåô</span>
</button>
<button class=nav-toggle id=navToggle aria-label="Toggle menu">‚ò∞</button></div></div></nav><main class=container><a class=back-link href=javascript:history.back()>‚Üê Back</a><article><h1>On Sparsity in Overparametrised Shallow ReLU Networks</h1><p><strong>NYU, MaD Group Meeting</strong> ‚Äî Seminar, 2020-10-19</p><h3 id=abstract>Abstract</h3><p>The analysis of neural network training beyond their linearization regime remains an outstanding open question, even in the simplest setup of a single hidden-layer. The limit of infinitely wide networks provides an appealing route forward through the mean-field perspective, but a key challenge is to bring learning guarantees back to the finite-neuron setting, where practical algorithms operate. Towards closing this gap, and focusing on shallow neural networks, in this work we study the ability of different regularisation strategies to capture solutions requiring only a finite amount of neurons, even on the infinitely wide regime. Specifically, we consider (i) a form of implicit regularisation obtained by injecting noise into training targets, and (ii) the variation-norm regularisation, compatible with the mean-field scaling. Under mild assumptions on the activation function (satisfied for instance with ReLUs), we establish that both schemes are minimised by functions having only a finite number of neurons, irrespective of the amount of overparametrisation. We study the consequences of such property and describe the settings where one form of regularisation is favorable over the other.</p></article></main><footer class=footer><p><a href=mailto:jdedios@nyu.edu>jdedios@nyu.edu</a> ¬∑ NYU Center for Data Science, 60 5th Ave, New York, NY 10011</p><p>&copy; 2026 Jaume de Dios Pont</p></footer><script>(function(){var e=document.getElementById("themeToggle"),n=e.querySelector(".theme-icon");function t(){var e=document.documentElement.getAttribute("data-theme")==="dark";n.textContent=e?"‚òÄÔ∏è":"üåô"}t(),e.addEventListener("click",function(){var e=document.documentElement.getAttribute("data-theme")==="dark";e?(document.documentElement.removeAttribute("data-theme"),localStorage.setItem("theme","light")):(document.documentElement.setAttribute("data-theme","dark"),localStorage.setItem("theme","dark")),t()})})(),document.getElementById("navToggle").addEventListener("click",function(){document.getElementById("navLinks").classList.toggle("open")})</script></body></html>